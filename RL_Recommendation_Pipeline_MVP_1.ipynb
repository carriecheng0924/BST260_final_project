{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMU+7rvmiVAXR5wPd72GNU9"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RL Recommendation Pipeline MVP 1.0\n",
        "This pipeline will run after the Evaluation pipeline and consider the following inputs and outputs:\n",
        "\n",
        "Inputs:\n",
        "\n",
        "*Context features*\n",
        "1.   Demographics ✅\n",
        "2.   Self-efficacy score change (composite score including emotion scores and emotion lexicon, to use the output from voice analysis + lexicon indicators) ✅\n",
        "3.   Consciousness score (conscientiousness model: content -> consicousness prob) ✅\n",
        "4.   Temporal info (time b/w suggested action and most recent activity - sugg.device.since) ✅\n",
        "\n",
        "*Feedback features*\n",
        "1.   User rating (as a proxy to whether this user has the intend to take this action) ✅\n",
        "2.   Activity mode (as a proxy to whether this user takes this action) ✅\n",
        "\n",
        "*Arms*\n",
        "1.   Messages/Actions (may consider to extract the actions from messages + other actions) ✅\n",
        "\n",
        "\n",
        "Outputs:\n",
        "1.   Probabilities for each action\n",
        "\n",
        "Post processing:\n",
        "1.   Pad the output probabilities with 0.2 for each unavailable action\n",
        "2.   Thompson sampling\n",
        "3.   Generate message for the selected action\n",
        "\n",
        "\n",
        "\n",
        "For MVP, we consider a LLM for conscientiousness model and a finitie-dimensional model (bayesian contextual bandit but with reward function being a nonlinear model)\n",
        "\n",
        "For the future, we should consider 1. causal inference applied in evaluation pipeline 2. deep reinforcement learning (DRL) framework\n"
      ],
      "metadata": {
        "id": "GixAjz1wGUX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install monai torchinfo pytorch-metric-learning"
      ],
      "metadata": {
        "id": "cISdq4srt9u2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGvcXvDwuCqD",
        "outputId": "2cea72cb-1581-4285-fcb7-98a3d001adfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load requirements & connect to drive\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import os\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix , classification_report\n",
        "from transformers import Trainer , TrainingArguments , BertTokenizer , BertForSequenceClassification\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopwords = set(stopwords.words(\"english\"))\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification , AutoTokenizer\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "# nltk.download('punkt')\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s06eLufftJ1k",
        "outputId": "b33f2ead-9b1a-44ff-bde7-ba779f9297d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(text) :\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    text = re.sub(r\"[^\\w\\s]\" , \"\" , text)\n",
        "\n",
        "    text = re.sub(r\"\\d+\" , \"\" , text)\n",
        "\n",
        "    words = text.split()\n",
        "\n",
        "    words = [w for w in words if w not in stopwords]\n",
        "\n",
        "    preprocessing = \" \".join(words)\n",
        "\n",
        "    return preprocessing\n",
        "\n",
        "def count_stopword(text):\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    text = re.sub(r\"[^\\w\\s]\" , \"\" , text)\n",
        "\n",
        "    text = re.sub(r\"\\d+\" , \"\" , text)\n",
        "\n",
        "    words = text.split()\n",
        "\n",
        "    nstopword = len([w for w in words if w in stopwords])\n",
        "\n",
        "    return nstopword\n",
        "\n",
        "def count_word(text):\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    text = re.sub(r\"[^\\w\\s]\" , \"\" , text)\n",
        "\n",
        "    text = re.sub(r\"\\d+\" , \"\" , text)\n",
        "\n",
        "    words = text.split()\n",
        "\n",
        "    nword = len(words)\n",
        "\n",
        "    return nword"
      ],
      "metadata": {
        "id": "OOOgxZrTEyaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bayesian Contextual Bandit with Nonlinear Reward Function - By 5/18"
      ],
      "metadata": {
        "id": "1KN3gl8J2fwT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to define the reward function and do some derivation for prior/posterior"
      ],
      "metadata": {
        "id": "hvXT2kw72q9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if we can have the distribution as a random function\n",
        "\n",
        "action space: A = {a_1, ..., a_k}\n",
        "R(a_i) ~ F_i\n",
        "\n",
        "Step 0: R needs to be defined - could be a composite of multiple metrics\n",
        "\n",
        "Step 1: Training\n",
        "given data points: d_1,...,d_n\n",
        "randomly sample p times with replacement\n",
        "R_j(a_i) = f_ij(X_j)\n",
        "...\n",
        "\n",
        "If there is no available data to train an arm, initialize the function\n",
        "R_j(a_s) = N(0, 1) - distribution can be chosen by users or just simply random(25%percentile of observed data, 50%percentile of observed data)\n",
        "\n",
        "Step 2: Sample from F_1,...,F_k and takes a = max(a_i)(R(a_1),...,R(a_k))\n",
        "given a data point,\n",
        "sample one function from f_ij's, estimate R_j(a_i), do p times\n",
        "R will be a k*p reward matrix for each individual, with rows indicating p samples from each arm, and columns being probs of arms from each sample. One can use R to compute P(a_i > a_j) = sum_s(1 to p)(R_s(a_i) > R_s(a_j))/p\n",
        "\n",
        "R_max = R.max(axis = 0)\n",
        "\n",
        "R_ind = R == rep(R_max, k).reshape(k, p)\n",
        "\n",
        "R_freq = R_ind.sum(axis = 0)\n",
        "\n",
        "Prob = R_freq/sum(R_freq)\n",
        "\n",
        "\n",
        "Guess: as long as the function used to estimate reward function in each random sample is Donsker, then f_i is Donsker, and we should be able to derive asymptotic properties, limit/convergence."
      ],
      "metadata": {
        "id": "Y4yzUNeS-xjr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pe1-5T2m2wl4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}